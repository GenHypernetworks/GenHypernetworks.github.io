<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="GenHyper">
  <meta name="keywords" content="GenHyper">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GenHyper</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="website/css/bulma.min.css">
  <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="website/css/bulma-slider.min.css">
  <link rel="stylesheet" href="website/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="website/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="website/js/fontawesome.all.min.js"></script>
  <script src="website/js/bulma-carousel.min.js"></script>
  <script src="website/js/bulma-slider.min.js"></script>
  <script src="website/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GenHyper : Adversarial Generative Hypernetworks for Efficient Policy Adaptation
            </h1>

            <div class="is-size-8 publication-authors">
              <!-- <span class="author-block">
                <a href="https://sanketkalwar.github.io/" target="_blank"><span class="looking">Sanket&nbsp;Kalwar</span></a><sup>*1</sup>,</span> -->
              <span class="author-block">
                <a href="http://www.linkedin.com/in/jayaram6111997" target="_blank">Jayaram&nbsp;Reddy</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://sanketkalwar.github.io/" target="_blank">Sanket&nbsp;Kalwar</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://anant-garg205.github.io/" target="_blank">Anant&nbsp;Garg</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                <a href="https://vishal-2000.github.io/" target="_blank">Vishal&nbsp;Mandadi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/brojeshwar/home" target="_blank">Brojeshwar&nbsp;Bhowmick</a><sup>2</sup>,
              </span>  
              <span class="author-block">
                <a href="https://sites.google.com/view/brojeshwar/home" target="_blank">Snehasis&nbsp;Banerjee</a><sup>2</sup>,
              </span>              
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=0zgDoIEAAAAJ" target="_blank">Arun&nbsp;Singh</a><sup>3</sup>,
              </span>

              <span class="author-block">
                <a href="https://www.iiit.ac.in/people/faculty/mkrishna/" target="_blank">Madhava&nbsp;Krishna</a><sup>1</sup>,
              </span>

            </div>

            <div class="is-size-9 publication-authors">
              <span class="author-block"><sup>1</sup>Robotic&nbsp;Research&nbsp;Center&nbsp;IIIT Hyderabad,
                <sup>2</sup>TCS&nbsp;Research&nbsp;India,
                <sup>3</sup>University&nbsp;of&nbsp;Tartu </span><br>
                <small><sup>*</sup>Equal Contribution. Authors ordered randomly</small> </span>
              <!-- <span class="author-block"><sup>2</sup>Cornell Tech, Cornell University &nbsp; </span> -->
            </div>
            
            <!-- <span class="looking"><small>Looking for MS/PhD positions for FY 2024!</small></span> -->

            <!-- <h1 style="font-size:16px;font-weight:bold">Accepted at ICRA 2024</h1> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="IROS25_GenHyper_submission_final.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/GenHyper/amt_hyp" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                  <span class="link-block">
                    <a href="https://youtu.be/SlhH8Nuy2UM" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube "></i>
                      </span>
                      <span>5 min Video</span>
                    </a>
                    <span class="link-block">
                      <a href="https://youtu.be/mnlnkla-l08" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-youtube "></i>
                        </span>
                        <span>3 min Video</span>
                      </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
      <iframe id="teaser" width="100%" height="100%" 
          src="https://www.youtube.com/embed/SlhH8Nuy2UM?autoplay=1&mute=1&loop=1&playsinline=1" 
          frameborder="0" 
          allow="autoplay; encrypted-media; picture-in-picture" 
          allowfullscreen>
      </iframe>
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://www.youtube.com/embed/SlhH8Nuy2UM" type="video/mp4">
      <source src="videos/IROS_FULL_website_2025.mp4" type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">GenHyper</span> Adversarial Generative Hypernetworks for Efficient Policy Adaptation
      </h2>
      </div>
    </div>
  </section>

  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">

        <div class="columns">
          <div class="item item-fullbody column" style="text-align: center;">
            <video poster="ocrtoc1" id="fullbody1" class="teaser-video" autoplay playsinline muted loop width="75%">
              <source src="videos/ocrtoc/ocrtoc1.mp4" type="video/mp4">
            </video>

            <video poster="ocrtoc2" id="fullbody2" class="teaser-video" autoplay playsinline muted loop width="75%">
              <source src="videos/ocrtoc/ocrtoc2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
          EDMP generalizes to arbitrary scenes and objects <b>directly at inference</b>!
        </h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/clutter1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/clutter5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/ocrtoc/ocrtoc1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/clutter2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/clutter3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="videos/clutter4.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/ocrtoc/ocrtoc2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/clutter6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/clutter7.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/clutter8.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  
    <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
          EDMP at the time of <b>training</b>!
        </h2> -->
        <!-- <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/train1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/train2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/train3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="videos/train4.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="videos/train5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="videos/train6.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Reinforcement Learning (RL) often requires large
              number of environment interactions to generalize to unseen
              in-distribution tasks, particularly when policy initialization is
              suboptimal. Existing meta-RL and transformer-based methods
              adapt to unseen tasks with few demonstrations but usually
              require training on many tasks (usually 85% of tasks in
              task distribution). To address this challenge, we propose a
              novel framework that leverages adversarial hypernetworks to
              generate strong policy initializations on unseen tasks, enabling
              rapid adaptation with minimal interactions, even when pre-
              trained on as minimum as 30% of tasks. We demonstrate the
              effectiveness of our approach on MuJoCo continuous control
              tasks, showcasing strong zero-shot policy initialization and
              rapid adaptation on unseen tasks. Additionally, we demonstrate
              that our framework can be extended to Multi-Task RL (MTRL)
              setting, where it outperforms existing hypernetwork based
              methods on manipulation tasks from MetaWorld benchmark.
              Through rigorous experimentation, we show that our frame-
              work outperforms the prior competitive baselines from in-
              context RL and meta RL on zero-shot transfer and enables
              efficient adaptation to unseen in-distribution tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <section class="section">
        <div class="container is-max-desktop">
    
          <div class="columns is-centered">
            <div class="column is-full-width">
              <h2 class="title is-3" style="text-align: center;">Overview</h2>
              <div class="content has-text-justified" style="position: relative; z-index: 0;">
                <img src="images/GenHyper_architecture.png">
          <!-- <p>
            <b>Architecture.</b> consists of HypLatent, an adversarial autoencoder that generates diverse task-
            conditioned latent policy parameters, and HypFormer, a single layer transformer that performs soft-weighted aggregation on
            these priors towards expert policy parameters.
          </p> -->
              </div>
              <div>
                <h2 class="title is-3" style="text-align: center;">Section 1: Zero Shot Initialization + Efficient Adaptation</h2>
                <!-- <h3 class="title is-3" style="text-align: center;">Twid</h3> -->
                <div class="content has-text-justified">
                  <p>
                    <b>Components:</b>
                    <ul>
                      <li><b>HypLatent.</b> An generative adversarial framework with generator trained to predict policy parameters given the MDP parameters of the given task as input. The discriminator trains in parallel with the generator to discriminate synthetic parameters from the ground truth parameters, thereby forcing the generator to learn to predict parameters closer to the ground truth.</li>
                      <img src="images/hyplatent.png">
                      <li><b>Auxillary Network (Q).</b> This is a neural network that learns to reconstruct the MDP parameters, given an output token from the discriminator. Through this network, we ensure that the information related to the task (represented by MDP parameters) is not lost as the network trains with time, reducing the chance of mode convergence and other associated local minima</li>
                      <img src="images/auxillary.png">
                    </ul>
                  </p>
                  <p>
                    <b>Zero-shot Policy Initialization.</b> For a given Mujoco Continuous environment, this framework is trained on a subset of tasks. The subset size is varied from 30% to 85% of the total tasks. The trained model is then tested on unseen tasks from the same environment. We show that the generator model is able to generalize zero-shot to unseen tasks, even when trained on as minimum as 30% of tasks. 
                  </p>
                  <p>
                    <b>Efficient Adaptation.</b> We propose to use TD-regularized actor-critic method to adapt the zero-shot policy to the unseen task. We show that the policy is able to adapt to the unseen task with minimal interactions.
                  </p>
                  <img src="images/efficient_adapt.png">
                  <p>We tested our framework on Mujoco's Hopper and Ant-Direction environments. Please refer to the paper for detailed section on experiments.</p>
                </div>
              </div>

              <div>
                <h2 class="title is-3" style="text-align: center;">Section 2: Extension to Multi-Task RL </h2>
                <!-- <h3 class="title is-3" style="text-align: center;">Twid</h3> -->
                <div class="content has-text-justified">
                  <p>
                    <b>MTRL Setting.</b> We consider an agent interacting with an environment to perform multiple tasks sampled from a task distribution. Multi-Task Reinforcement Learning (MTRL) can be modeled as a Block Contextual Markov Decision Process (BC-MDP), where the context space represents shared information across tasks, the state space is universal, the action space is common, and each context maps to its specific task parameters. The goal of MTRL is to learn a single policy that maximizes the cumulative expected return across all tasks. While tasks share a universal state space, individual tasks may have distinct state spaces, as seen in benchmarks like MetaWorld, where variations arise due to differences in object interactions and behaviors.
                  </p>   
                  
                  <p>
                    <b>Components:</b>
                    <ul>
                      <li><b>Autoencoder.</b> For MTRL tasks, instead of directly training the generator to predict policy parameters, we choose to operate in the latent space, similar to the paper <a href="https://cheryyunl.github.io/make-an-agent/" class="uline-special"><span style="color:crimson">Make-An-Agent</span></a>. For this, we use an autoencoder to encode the policy neural network parameters as latent embeddings. The encoder is trained to predict latent embeddings given the policy parameters, and the decoder is trained to reconstruct the policy parameters given the latent embeddings. The autoencoder is trained with a reconstruction loss and a KL divergence loss to ensure that the latent embeddings are close to a standard normal distribution.</li>
                      <li><b>HypLatent.</b> An generative adversarial framework with generator trained to predict latent embeddings of policy parameters (unlike the previous section where the generator predicted policy parameters directly). The discriminator trains in parallel with the generator to discriminate synthetic latent embeddings from the ground truth latent embeddings, thereby forcing the generator to learn to predict latent embeddings closer to the ground truth.</li>
                      <li><b>Using Behavior Embeddings to Represent the Task.</b> Unlike previous section, where each task is represented by its MDP parameters, we choose to represent the task by its behavior embeddings, as it was proved to be more efficient representation for MTRL tasks (citing <a href="https://cheryyunl.github.io/make-an-agent/" class="uline-special"><span style="color:crimson">Make-An-Agent</span></a>). Thus, HypLatent's generator takes the behavior embedding as input, instead of MDP parameters, for MTRL tasks.</li>
                      <li><b>Auxillary Network (Q).</b> This is a neural network that learns to reconstruct the behavior embeddings (unlike the previous section, where it reconstructs MDP parameters of the task), given an output token from the discriminator. Through this network, we ensure that the information related to task (represented by behavior embeddings) is not lost as the network trains with time, reducing the chance of mode convergence and other associated local minima</li>
                      <li><b>HypFormer.</b> The policy parameters predicted by the generator/hypernetwork may not always be close to the ground truth policy embedding. To enhance accuracy, we perform soft-weighted aggregation, prioritizing latent embeddings that are closer to the ground truth. Directly regressing the refined embeddings from HypFormer led to unstable training; thus, we introduce two MLP branches: the latent head and the residue head. The latent head is trained to predict the ground truth latent policy embedding, while the residue head learns the residual between the generator's latent policy embedding and the ground truth. To ensure consistency between these heads, we apply a consistency loss. For a detailed explanation of individual loss terms and notations, please refer to Section V(b) of the paper. </li>
                      <img src="images/mtrl_arch.png">
                    </ul>
                  </p>
               
                  <!-- <p>
                    <b>Adversarial Hypernetworks.</b> We propose a novel framework that leverages adversarial hypernetworks to generate strong policy initializations on unseen tasks, enabling rapid adaptation with minimal interactions, even when pre-trained on as minimum as 30% of tasks.
                  </p>
                  <p>
                    <b>Hypernetworks.</b> Our architecture consists of HypLatent, an adversarial autoencoder that generates diverse task-conditioned latent policy parameters, and HypFormer, a single layer transformer that performs soft-weighted aggregation on these priors towards expert policy parameters.
                  </p> -->
                </div>
            </div>
          </div>
        </div>
      </section>
      
    <!-- <section>
    	<h2 class="title is-3" style="text-align: center; margin-bottom:-25px">Results</h2>
    
    <div class="hero-body edmp-diff-vis">
      <div class="container">
        <h2 class="subtitle has-text-centered">
          Visualizing EDMP's diffusion steps at the time of inference</b>!
        </h2>
        <div id="results-carousel" class="carousel results-carousel"> -->
          <!-- <br> -->
          <!-- <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="300%">
                <source src="videos/denoisings/denoising_video_speedened1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
              <source src="videos/denoisings/denoising_video_speedened2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
                <source src="videos/denoisings/denoising_video_speedened3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
                <source src="videos/denoisings/denoising_video_speedened4.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
                <source src="videos/denoisings/denoising_video_speedened5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay muted loop playsinline height="140%">
              <source src="videos/denoisings/denoising_video_speedened6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
                <source src="videos/denoisings/denoising_video_speedened7.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay muted loop playsinline height="140%">
              <source src="videos/denoisings/denoising_video_speedened8.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
       -->


      <!--<section class="section" style="margin-bottom: -80px;">
        <div class="container is-max-desktop">

          <div class="columns is-centered has-text-centered" style="margin-bottom: -30px;">
            <h2 class="title is-3">Ensemble of costs guided diffusion</h2>
            <h2 class="title is-3"> Visualizing diffusion-steps at the time of inference!</h2>
          </div>
        
          <div class="hero-body">
            <center>
              <video id="teaser" autoplay muted loop playsinline width="99%">
                <source src="videos/icra_24_single_classifier_24P_h264.mp4" type="video/mp4">
              </video>
            </center>
          </div>
        </div>
      </section>-->

      <!-- <section class="hero teaser">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
            	<h2 class="subtitle has-text-centered">
		  EDMP generalizes to any scene <b>without (left)</b> and <b>with (right)</b> holding objects!
		</h2>
              <video id="teaser" autoplay muted loop playsinline width="60%" style="mid-width: 370px">
                <source src="videos/with-without-grasp-clipped.mp4" type="video/mp4">
              </video>
            </center>
          </div>
        </div>
      </section>
      
        <section class="hero teaser">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
            	<h2 class="subtitle has-text-centered">
		  Based on diffusion, EDMP generates multimodal trajectories! .. As we show in the paper, the ensemble-of-costs further improves the multimodality!
		</h2>
              <video id="teaser" autoplay muted loop playsinline width="40%" style="min-width:310px">
                <source src="videos/MM_v2.mp4" type="video/mp4">
              </video>
            </center>
          </div>
        </div>
      </section> -->

  <!--/ Paper video. -->
  <!-- <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h3 class="title is-3">Acknowledgements</h3>
      <div class="content has-text-justified">
        <p>
          We would like to thank Adam Fishman for assisting with MÏ€Nets and providing valuable insights into collision checking and benchmarking.
        </p>
      </div>
    </div>
  </div>
  </div>
  </section> -->
<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{saha2023edmp,
      title={EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning}, 
      author={Kallol Saha and Vishal Mandadi and Jayaram Reddy and Ajit Srikanth and Aditya Agarwal and Bipasha Sen and Arun Singh and Madhava Krishna},
      year={2023},
      eprint={2309.11414},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
    </code></pre>
    </div>
  </section>
   -->
  <div hidden="hidden">
  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=2oCrnzNWBNsmNO2WGMHvLpem_9_jluEUy9J8i0GRHJw"></script>
  </div>


  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          <br> The source code for this website is borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
